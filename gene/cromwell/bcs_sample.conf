include required(classpath("application"))
database {
    profile = "slick.jdbc.MySQLProfile$"
    db {
        driver = "com.mysql.cj.jdbc.Driver"
        url = "jdbc:mysql://<rds
        address>/db_cromwell?rewriteBatchedStatements=true&useSSL=false&allowPublicKeyRetriev
        al=true"
        # MySQL username and password
        user = "user_cromwell"
        password = "HEllo1234"
        connectionTimeout = 5000
    }
}

workflow-options {
    workflow-log-dir = "/home/cromwell/cromwell/logs/"
}

call-caching {
    # Allows re-use of existing results for jobs you've already run
    # (default: false)
    enabled = true
    # Whether to invalidate a cache result forever if we cannot reuse them. Disable this if you
    expect some cache copies
    # to fail for external reasons which should not invalidate the cache (e.g. auth differences
    between users):
    # (default: true)
    invalidate-bad-cache-results = true
    # blacklist-cache {
    #   # The call caching blacklist cache is off by default. This cache is used to blacklist cache
    hit paths based on the
    #   # prefixes of cache hit paths that Cromwell has previously failed to copy for permissions
    reasons.
    #   enabled: true
    #   # Guava cache concurrency.
    #   concurrency: 10000
    #   # How long entries in the cache should live from the time of their last access.
    #   ttl: 20 minutes
    #   # Maximum number of entries in the cache.
    #   size: 1000
    # }
}

docker {
    hash-lookup {
        enable = true
        # Set this to match your available quota against the Google Container Engine API
        #gcr-api-queries-per-100-seconds
        = 1000
        # Time in minutes before an entry expires from the docker hashes cache and needs to be
        fetched again
        #cache-entry-ttl = "20 minutes"
        # Maximum number of elements to be kept in the cache. If the limit is reached, old elements
        will be removed from the cache
        #cache-size = 200
        # How should docker hashes be looked up. Possible values are "local" and "remote"
        # "local": Lookup hashes on the local docker daemon using the cli
        # "remote": Lookup hashes on docker hub and gcr
        method = "remote"
        #method = "local"
        alibabacloudcr {
            num-threads = 5
            auth {
                access-id = ""
                access-key = ""
            }
        }
    }
}

    engine {
    filesystems {
    oss {
    auth {
    endpoint = "oss-cn-zhangjiakou-internal.aliyuncs.com"
    access-id = ""
    access-key = ""
    }
    }
    }
    }
    backend {
    default = "BCS"
    providers {
    BCS {
    actor-factory = "cromwell.backend.impl.bcs.BcsBackendLifecycleActorFactory"
    config {
    root = "oss://<bucket name>/cromwell_dir"
    region = "cn-zhangjiakou"
    access-id = ""
    access-key = ""
    filesystems {
    oss {
    auth {
    endpoint = "oss-cn-zhangjiakou-internal.aliyuncs.com"
    access-id = ""
    access-key = ""
    }
    caching {
    # When a cache hit is found, the following duplication strategy will be
    followed to use the cached outputs
    # Possible values: "copy", "reference". Defaults to "copy"
    # "copy": Copy the output files
    # "reference": DO NOT copy the output files but point to the original output
    files instead.
    #              Will still make sure than all the original output files exist and
    are accessible before
    #              going forward with the cache hit.
    duplication-strategy = "reference"
    }
    }
    }
    default-runtime-attributes {
    failOnStderr: false
    continueOnReturnCode: 0
    autoReleaseJob: false
    ## Auto Cluster
    cluster: "OnDemand ecs.c5.4xlarge img-ubuntu-vpc"
    systemDisk: "cloud_efficiency 40"
    vpc: "192.168.0.0/16"
    ## Cluster
    #  cluster: cls-1dunl6pun5pgdsjts0k000
    }
    }
    }
    }
}